场景需求：worcount实现   很多情况下任务都是一个WordCount的延伸，比如说搜索中优先匹配，可以直接统计出现的关键字次数，然后进行  统计输出出现次数最高的。  当数据量不大时候可以使用shell的方式进行统计，但是当数据量达到PB等级别时候，就无法使用，这个时候就需  要通过分布式处理技术，MapReduce来进行处理。  MapReduce的主要核心部分就是Map、Reduce，主要过程就是：  1. 先将输入的数据进行Spitting操作，将一个ie文件拆分成多个块，并且支持并行处理  2. 然后进行Mapping操作，主要是将数据实现简单统计，形成Key-Value对，这里Key是单词，Value是1  3. 最关键的一步也是影响性能最大的部分就是Shuffling，将多台机器上的相同key统计到一台机器  4. 进行reduce操作，统计每个Key出现的次数总和  5. 输出最终结果  核心概念：  * Split  指的就是交由MapReduce作业处理的数据块，是MapReduce中最小的计算单元，与其对应的就是HDFS中的block  概念，block默认的大小是128M，也是HDFS最小的存储单元  